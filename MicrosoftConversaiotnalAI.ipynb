{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Investigating Conversational AI for VSim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO DO'S\n",
    "\n",
    "- if sound is unclear give proper responds\n",
    "- if question not understood give proper responds\n",
    "- Question: What if more matches happen? different matches?\n",
    "- Can we save data for training more complex models? GPDR\n",
    "- how should sound files be merged\n",
    "- Medical terms\n",
    "- Scalability? https://azure.microsoft.com/da-dk/pricing/details/cognitive-services/speech-services/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "Import libraries and write settings here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing language model...\n",
      "done..\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "import json\n",
    "import os\n",
    "import spacy\n",
    "import random\n",
    "from spacy.tokens import Token\n",
    "from playsound import playsound\n",
    "\n",
    "\n",
    "#Load the Language Model\n",
    "print('Initializing language model...')\n",
    "nlp = spacy.load('en_core_web_lg') #large language model\n",
    "#nlp = spacy.load('en_core_web_sm') #small language model\n",
    "print('done..')\n",
    "#Set credential for Microsft Cog Services\n",
    "region = \"eastus\"\n",
    "subkey = \"3f7c2e668a5a4f3ca4923ff200e2db8b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get paths and general questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Questions Are ['AreYouAllergicToAnything', 'CanYouDescribeThePain', 'DoYouHaveAnyPain', 'HowAreYouFeeling', 'PleaseStateYourNameAndDateOfBirth', 'WhatDoYouNeed']\n"
     ]
    }
   ],
   "source": [
    "questions = []\n",
    "pathToSoundFiles = []\n",
    "answerDict = {}\n",
    "\n",
    "#Getting dirs(what is named as the questions) and reponds.wav paths\n",
    "for root, dirs, files in os.walk(\"./static/SoundFilesCAIPrototype/\", topdown=False):\n",
    "    for name in dirs:\n",
    "        questions.append(name)\n",
    "        answerDict[name] = {} \n",
    "   \n",
    "    for name in files:\n",
    "        p = os.path.join(root, name)      \n",
    "        pathToSoundFiles.append(p)\n",
    "      \n",
    "#Creating a dict where each question contains the path for the proper responses  \n",
    "for k in answerDict.keys():\n",
    "    k_list = [key for key in pathToSoundFiles if k in key]\n",
    "    \n",
    "    answerDict[k] = k_list\n",
    "    \n",
    "print('The Questions Are', questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapEventToQuesion = { 'AnyAllergies' : 'AreYouAllergicToAnything',\n",
    "                     'DescirbePain' : 'CanYouDescribeThePain',\n",
    "                      'HowFeel':'HowAreYouFeeling',\n",
    "                      'NameAndBirth' : 'PleaseStateYourNameAndDateOfBirth',\n",
    "                      'WhatYouNeed' : 'WhatDoYouNeed',\n",
    "                      'AnyPain' : 'DoYouHaveAnyPain'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_transcription(string):\n",
    "    import uuid\n",
    "    root_path = './transcriptions/'\n",
    "    filename = root_path + str(uuid.uuid1())+'.txt'\n",
    "    text_file = open(filename, \"w\")\n",
    "    n = text_file.write(string)\n",
    "    text_file.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json(string, label):\n",
    "    import uuid\n",
    "    import json\n",
    "    root_path = './transcriptions/'\n",
    "    filename = root_path + str(uuid.uuid1())+'.json'\n",
    "    data = {'transcription': string ,'label': label}\n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(data, fp)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def see_text_files():\n",
    "    text_paths = []\n",
    "    strings = []\n",
    "    for root, dirs, files in os.walk(\"./transcriptions/\", topdown=False):\n",
    "        for name in files:\n",
    "            p = os.path.join(root, name)      \n",
    "            text_paths.append(p)\n",
    "    for t in text_paths:\n",
    "        f = open(t, \"r\")\n",
    "        strings.append(f.read())\n",
    "    return strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def what_match(d, matcher):\n",
    "    matches=matcher(d)\n",
    "    if matches:\n",
    "        return doc.vocab.strings[matches[0][0]]\n",
    "    else:\n",
    "        return 'The question is not recognized. Please ask again.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_doc(doc):\n",
    "    from spacy import displacy\n",
    "    displacy.render(doc, style=\"dep\", jupyter=True)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sound_files(path_file1, path_file2, dest_path):\n",
    "    from pydub import AudioSegment\n",
    "\n",
    "    sound1 = AudioSegment.from_mp3(path_file1)\n",
    "    sound2 = AudioSegment.from_mp3(path_file2)\n",
    "\n",
    "    combined_sounds = sound1 + sound2\n",
    "    combined_sounds.export(dest_path, format=\"mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Bot or Question Inten Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Can you please state your name and the day you were born',\n",
       " 'ents': [{'start': 35, 'end': 42, 'label': 'DATE'}],\n",
       " 'sents': [{'start': 0, 'end': 56}],\n",
       " 'tokens': [{'id': 0,\n",
       "   'start': 0,\n",
       "   'end': 3,\n",
       "   'pos': 'VERB',\n",
       "   'tag': 'MD',\n",
       "   'dep': 'aux',\n",
       "   'head': 3},\n",
       "  {'id': 1,\n",
       "   'start': 4,\n",
       "   'end': 7,\n",
       "   'pos': 'PRON',\n",
       "   'tag': 'PRP',\n",
       "   'dep': 'nsubj',\n",
       "   'head': 3},\n",
       "  {'id': 2,\n",
       "   'start': 8,\n",
       "   'end': 14,\n",
       "   'pos': 'INTJ',\n",
       "   'tag': 'UH',\n",
       "   'dep': 'intj',\n",
       "   'head': 3},\n",
       "  {'id': 3,\n",
       "   'start': 15,\n",
       "   'end': 20,\n",
       "   'pos': 'VERB',\n",
       "   'tag': 'VB',\n",
       "   'dep': 'ROOT',\n",
       "   'head': 3},\n",
       "  {'id': 4,\n",
       "   'start': 21,\n",
       "   'end': 25,\n",
       "   'pos': 'DET',\n",
       "   'tag': 'PRP$',\n",
       "   'dep': 'poss',\n",
       "   'head': 5},\n",
       "  {'id': 5,\n",
       "   'start': 26,\n",
       "   'end': 30,\n",
       "   'pos': 'NOUN',\n",
       "   'tag': 'NN',\n",
       "   'dep': 'dobj',\n",
       "   'head': 3},\n",
       "  {'id': 6,\n",
       "   'start': 31,\n",
       "   'end': 34,\n",
       "   'pos': 'CCONJ',\n",
       "   'tag': 'CC',\n",
       "   'dep': 'cc',\n",
       "   'head': 3},\n",
       "  {'id': 7,\n",
       "   'start': 35,\n",
       "   'end': 38,\n",
       "   'pos': 'DET',\n",
       "   'tag': 'DT',\n",
       "   'dep': 'det',\n",
       "   'head': 8},\n",
       "  {'id': 8,\n",
       "   'start': 39,\n",
       "   'end': 42,\n",
       "   'pos': 'NOUN',\n",
       "   'tag': 'NN',\n",
       "   'dep': 'conj',\n",
       "   'head': 3},\n",
       "  {'id': 9,\n",
       "   'start': 43,\n",
       "   'end': 46,\n",
       "   'pos': 'PRON',\n",
       "   'tag': 'PRP',\n",
       "   'dep': 'nsubjpass',\n",
       "   'head': 11},\n",
       "  {'id': 10,\n",
       "   'start': 47,\n",
       "   'end': 51,\n",
       "   'pos': 'AUX',\n",
       "   'tag': 'VBD',\n",
       "   'dep': 'auxpass',\n",
       "   'head': 11},\n",
       "  {'id': 11,\n",
       "   'start': 52,\n",
       "   'end': 56,\n",
       "   'pos': 'VERB',\n",
       "   'tag': 'VBN',\n",
       "   'dep': 'relcl',\n",
       "   'head': 8}]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Question 1\n",
    "doc = nlp('Can you please state your name and the day you were born')\n",
    "doc.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allergies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "anyAllergies = [\n",
    "    {'POS': {\"IN\": [\"AUX\",\"PART\"]}},\n",
    "    {'POS': 'PRON'},\n",
    "    {'POS': {\"IN\": [\"AUX\",\"PART\",\"DET\"]}, 'OP': '*'},\n",
    "    {'lemma': {\"IN\": ['allergic','allergy','hypersensitivity']}}  \n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Can you describe the pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "DescirbePain = [\n",
    "    {'POS': {\"IN\": ['PRON']}},\n",
    "    {'POS': {\"IN\": [\"AUX\",\"PART\",\"DET\"]}, 'OP': '*'},\n",
    "    {'lemma': {'IN' : ['describe','characterize','explain','identify']}},\n",
    "    {'POS': {\"IN\": [\"AUX\",\"PART\",\"DET\"]}, 'OP': '*'},\n",
    "    {'lemma': {'IN' : ['pain']}}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do you have any pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AnyPain = [\n",
    "    {'POS': {\"IN\": ['PRON']}},\n",
    "    {'lemma': {'IN' : ['have','are']}, 'OP': '?'},\n",
    "    {'POS': {\"IN\": [\"AUX\",\"PART\",\"DET\",\"ADP\"]}, 'OP': '*'},\n",
    "    {'lemma': {'IN' : ['pain']}}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How are you feeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "HowFeel = [\n",
    "    {'lemma': 'how'},\n",
    "    {'POS': 'AUX'},\n",
    "    {'POS': { 'IN' : ['PRON','DET']}},\n",
    "    {'lemma': {'IN' : ['feel','do','go']}, 'OP' : '?'}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please state your name and date of birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "NameAndBirth = [\n",
    "    {'lower': 'your'},\n",
    "    {'lemma': 'name'},\n",
    "    {'POS': {\"IN\": [\"AUX\",\"PART\",\"DET\",\"ADP\",\"NOUN\",'CCONJ','PRON']}, 'OP': '*'},\n",
    "    {'ORTH': {'IN' : ['birth','birthday','born']}}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "WhatYouNeed = [\n",
    "    {'lower': 'what'},\n",
    "    {'lemma': {'IN' : ['do','is']}},\n",
    "    {'POS': {\"IN\": [\"AUX\",\"PART\",\"DET\",\"ADP\",\"NOUN\",'CCONJ','PRON']}, 'OP': '*'},\n",
    "    {'lemma': {'IN' : ['need','want','require']}}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched span: your name and the day you were born\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'NameAndBirth'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "matcher.add('AnyAllergies', None, anyAllergies )\n",
    "matcher.add('DescirbePain', None, DescirbePain )\n",
    "matcher.add('AnyPain', None, AnyPain )\n",
    "matcher.add('HowFeel', None, HowFeel )\n",
    "matcher.add('NameAndBirth', None, NameAndBirth )\n",
    "matcher.add('WhatYouNeed', None, WhatYouNeed )\n",
    "\n",
    "\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print('matched span:', matched_span.text)\n",
    "what_match(doc, matcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"0b9f1b86ac51426d9f52751c3b43a745-0\" class=\"displacy\" width=\"2150\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Can</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">you</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">please</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">INTJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">state</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">your</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">name</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">day</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">you</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">were</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">born</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0b9f1b86ac51426d9f52751c3b43a745-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,89.5 570.0,89.5 570.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0b9f1b86ac51426d9f52751c3b43a745-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0b9f1b86ac51426d9f52751c3b43a745-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,177.0 565.0,177.0 565.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0b9f1b86ac51426d9f52751c3b43a745-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0b9f1b86ac51426d9f52751c3b43a745-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,264.5 560.0,264.5 560.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0b9f1b86ac51426d9f52751c3b43a745-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">intj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0b9f1b86ac51426d9f52751c3b43a745-0-3\" stroke-width=\"2px\" d=\"M770,352.0 C770,264.5 910.0,264.5 910.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0b9f1b86ac51426d9f52751c3b43a745-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,354.0 L762,342.0 778,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0b9f1b86ac51426d9f52751c3b43a745-0-4\" stroke-width=\"2px\" d=\"M595,352.0 C595,177.0 915.0,177.0 915.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0b9f1b86ac51426d9f52751c3b43a745-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M915.0,354.0 L923.0,342.0 907.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0b9f1b86ac51426d9f52751c3b43a745-0-5\" stroke-width=\"2px\" d=\"M595,352.0 C595,89.5 1095.0,89.5 1095.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0b9f1b86ac51426d9f52751c3b43a745-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1095.0,354.0 L1103.0,342.0 1087.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0b9f1b86ac51426d9f52751c3b43a745-0-6\" stroke-width=\"2px\" d=\"M1295,352.0 C1295,264.5 1435.0,264.5 1435.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0b9f1b86ac51426d9f52751c3b43a745-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1295,354.0 L1287,342.0 1303,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0b9f1b86ac51426d9f52751c3b43a745-0-7\" stroke-width=\"2px\" d=\"M595,352.0 C595,2.0 1450.0,2.0 1450.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0b9f1b86ac51426d9f52751c3b43a745-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1450.0,354.0 L1458.0,342.0 1442.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0b9f1b86ac51426d9f52751c3b43a745-0-8\" stroke-width=\"2px\" d=\"M1645,352.0 C1645,177.0 1965.0,177.0 1965.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0b9f1b86ac51426d9f52751c3b43a745-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,354.0 L1637,342.0 1653,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0b9f1b86ac51426d9f52751c3b43a745-0-9\" stroke-width=\"2px\" d=\"M1820,352.0 C1820,264.5 1960.0,264.5 1960.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0b9f1b86ac51426d9f52751c3b43a745-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,354.0 L1812,342.0 1828,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0b9f1b86ac51426d9f52751c3b43a745-0-10\" stroke-width=\"2px\" d=\"M1470,352.0 C1470,89.5 1970.0,89.5 1970.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0b9f1b86ac51426d9f52751c3b43a745-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1970.0,354.0 L1978.0,342.0 1962.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_doc(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STT Module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_key, service_region = subkey, region\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)\n",
    "\n",
    "#speech_config.request_word_level_timestamps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, my name is Daniel Beck Hansen. I'm born in Denmark.\n"
     ]
    }
   ],
   "source": [
    "result = speech_recognizer.recognize_once()\n",
    "print(result.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConversationalModel(nlp=nlp, matcher=matcher, speech_recognizer=speech_recognizer):\n",
    "    print('Start Speaking..')\n",
    "    result = speech_recognizer.recognize_once()\n",
    "    print('Recognized:',result.text)\n",
    "    print('Matched event:', what_match(nlp(result.text),matcher))\n",
    "    \n",
    "    if what_match(nlp(result.text),matcher) == 'AnyAllergies':\n",
    "        playsound(random.choice(answerDict[questions[0]]))\n",
    "    elif what_match(nlp(result.text),matcher) == 'DescirbePain':\n",
    "        playsound(random.choice(answerDict[questions[1]]))\n",
    "    elif what_match(nlp(result.text),matcher) == 'AnyPain':\n",
    "        playsound(random.choice(answerDict[questions[2]]))\n",
    "    elif what_match(nlp(result.text),matcher) == 'HowFeel':\n",
    "        playsound(random.choice(answerDict[questions[3]]))\n",
    "    elif what_match(nlp(result.text),matcher) == 'NameAndBirth':\n",
    "        playsound(random.choice(answerDict[questions[4]]))\n",
    "    elif what_match(nlp(result.text),matcher) == 'WhatYouNeed':\n",
    "        playsound(random.choice(answerDict[questions[5]]))\n",
    "    else:\n",
    "        print('Question not recognized')\n",
    "    \n",
    "    write_transcription(result.text)\n",
    "    write_json(result.text, what_match(nlp(result.text),matcher))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Speaking..\n",
      "Recognized: Do you have any allergies?\n",
      "Matched event: AnyAllergies\n"
     ]
    }
   ],
   "source": [
    "ConversationalModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TTS Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_config = speechsdk.AudioOutputConfig(filename=\"./test1.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.cognitiveservices.speech.ResultFuture at 0x27236a04358>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "synthesizer.speak_text_async(\"A simple test to write to a file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.cognitiveservices.speech.ResultFuture at 0x2550cc97f98>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output sound directly\n",
    "# audio_config = speechsdk.AudioOutputConfig(use_default_speaker=True)\n",
    "# synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "# synthesizer.speak_text_async(\"A simple test to write to a file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=None)\n",
    "result = synthesizer.speak_text_async(\"Getting the response as an in-memory stream.\").get()\n",
    "stream = speechsdk.AudioDataStream(result)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "audio_config = speechsdk.AudioOutputConfig(filename=\"./test1.wav\")\n",
    "synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config, audio_config=audio_config)\n",
    "synthesizer.speak_ssml_async(\"A simple test to write to a file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Show graphs and stats here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = speech_recognizer.recognize_once()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['need',\n",
       " 'Need',\n",
       " 'NEED',\n",
       " 'WANT',\n",
       " 'wAnt',\n",
       " 'want',\n",
       " 'Want',\n",
       " 'NEEDED',\n",
       " 'Needed',\n",
       " 'needed',\n",
       " 'CAn',\n",
       " 'cAN',\n",
       " 'CaN',\n",
       " 'can',\n",
       " 'cAn',\n",
       " 'Can',\n",
       " 'CAN',\n",
       " 'Needs',\n",
       " 'NEEDS',\n",
       " 'needs',\n",
       " 'NEEDs',\n",
       " 'SHOULD',\n",
       " 'SHould',\n",
       " 'should',\n",
       " 'Should',\n",
       " 'Get',\n",
       " 'gEt',\n",
       " 'GET',\n",
       " 'gET',\n",
       " 'get',\n",
       " 'GEt',\n",
       " 'GeT',\n",
       " 'ABLE',\n",
       " 'able',\n",
       " 'Able',\n",
       " 'YoU',\n",
       " 'You',\n",
       " 'YOU',\n",
       " 'yoU',\n",
       " 'you',\n",
       " 'yOU',\n",
       " 'yOu',\n",
       " 'YOu',\n",
       " 'MAKE',\n",
       " 'make',\n",
       " 'Make',\n",
       " 'mAke',\n",
       " 'MAke',\n",
       " 'SURE',\n",
       " 'Sure',\n",
       " 'sure',\n",
       " 'SUre',\n",
       " 'do',\n",
       " 'Do',\n",
       " 'DO',\n",
       " 'dO',\n",
       " 'GIVE',\n",
       " 'give',\n",
       " 'Give',\n",
       " 'GIve',\n",
       " 'KNOW',\n",
       " 'knOw',\n",
       " 'Know',\n",
       " 'know',\n",
       " 'KNow',\n",
       " 'HAve',\n",
       " 'havE',\n",
       " 'haVe',\n",
       " 'have',\n",
       " 'hAVE',\n",
       " 'Have',\n",
       " 'HAVE',\n",
       " 'hAve',\n",
       " 'HaVe',\n",
       " 'Needing',\n",
       " 'needing',\n",
       " 'NEEDING',\n",
       " 'COULD',\n",
       " 'COuld',\n",
       " 'Could',\n",
       " 'could',\n",
       " \"'ll\",\n",
       " \"'LL\",\n",
       " \"'Ll\",\n",
       " 'Must',\n",
       " 'must',\n",
       " 'MUST',\n",
       " 'WOuld',\n",
       " 'wOuld',\n",
       " 'WOULD',\n",
       " 'would',\n",
       " 'Would',\n",
       " \"N'T\",\n",
       " \"n't\",\n",
       " \"n'T\",\n",
       " \"N't\",\n",
       " 'Help',\n",
       " 'help',\n",
       " 'HELP',\n",
       " 'might',\n",
       " 'Might',\n",
       " 'MIGHT',\n",
       " 'MIght',\n",
       " 'KEEP',\n",
       " 'Keep',\n",
       " 'keep',\n",
       " 'ENOUGH',\n",
       " 'Enough',\n",
       " 'enough',\n",
       " 'understand',\n",
       " 'Understand',\n",
       " 'UNDERSTAND',\n",
       " 'They',\n",
       " 'they',\n",
       " 'THEY',\n",
       " 'THey',\n",
       " 'theY',\n",
       " 'WILL',\n",
       " 'WiLL',\n",
       " 'will',\n",
       " 'WIll',\n",
       " 'Will',\n",
       " 'TAKE',\n",
       " 'take',\n",
       " 'tAke',\n",
       " 'Take',\n",
       " 'TaKe',\n",
       " 'necessary',\n",
       " 'Necessary',\n",
       " 'NECESSARY',\n",
       " 'we',\n",
       " 'We',\n",
       " 'WE',\n",
       " 'wE',\n",
       " 'require',\n",
       " 'Require',\n",
       " 'REQUIRE',\n",
       " 'LeT',\n",
       " 'Let',\n",
       " 'let',\n",
       " 'LET',\n",
       " 'LEt',\n",
       " 'CANNOT',\n",
       " 'cannot',\n",
       " 'Cannot',\n",
       " 'canNOT',\n",
       " 'Put',\n",
       " 'PUT',\n",
       " 'put',\n",
       " 'noT',\n",
       " 'not',\n",
       " 'Not',\n",
       " 'NOt',\n",
       " 'nOt',\n",
       " 'NoT',\n",
       " 'nOT',\n",
       " 'NOT',\n",
       " 'willing',\n",
       " 'WILLING',\n",
       " 'Willing',\n",
       " 'WANTING',\n",
       " 'wanting',\n",
       " 'Wanting',\n",
       " 'IF',\n",
       " 'If',\n",
       " 'İf',\n",
       " 'if',\n",
       " 'iF',\n",
       " 'BETTER',\n",
       " 'Better',\n",
       " 'betteR',\n",
       " 'better',\n",
       " 'FInd',\n",
       " 'Find',\n",
       " 'FIND',\n",
       " 'find',\n",
       " 'way',\n",
       " 'wAy',\n",
       " 'Way',\n",
       " 'WAY',\n",
       " 'CONSIDER',\n",
       " 'consider',\n",
       " 'Consider',\n",
       " 'BRING',\n",
       " 'Bring',\n",
       " 'bring',\n",
       " 'bE',\n",
       " 'be',\n",
       " 'BE',\n",
       " 'Be',\n",
       " 'HoW',\n",
       " 'how',\n",
       " 'hOw',\n",
       " 'HOw',\n",
       " 'hOW',\n",
       " 'HOW',\n",
       " 'How',\n",
       " 'hoW',\n",
       " 'TRy',\n",
       " 'TRY']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "ms = nlp.vocab.vectors.most_similar(numpy.asarray([nlp.vocab.vectors[nlp.vocab.strings['need']]]), n=200)\n",
    "[nlp.vocab.strings[w] for w in ms[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can't find key natural_language_processing|NOUN in table",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5c0cfee5c8b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms2v_freq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mvector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms2v_vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mmost_similar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms2v_most_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;31m# [(('machine learning', 'NOUN'), 0.8986967),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m#  (('computer vision', 'NOUN'), 0.8636297),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sense2vec\\component.py\u001b[0m in \u001b[0;36ms2v_most_similar\u001b[1;34m(self, obj, n)\u001b[0m\n\u001b[0;32m    187\u001b[0m         \"\"\"\n\u001b[0;32m    188\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms2v_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_s2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ms2v\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit_key\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sense2vec\\sense2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, keys, n, batch_size)\u001b[0m\n\u001b[0;32m    212\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 214\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Can't find key {key} in table\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    215\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"indices\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can't find key natural_language_processing|NOUN in table"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from sense2vec import Sense2VecComponent\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "s2v = Sense2VecComponent(nlp.vocab)\n",
    "nlp.add_pipe(s2v)\n",
    "\n",
    "doc = nlp(\"A sentence about natural language processing.\")\n",
    "assert doc[3:6].text == \"natural language processing\"\n",
    "freq = doc[3:6]._.s2v_freq\n",
    "vector = doc[3:6]._.s2v_vec\n",
    "most_similar = doc[3:6]._.s2v_most_similar(3)\n",
    "# [(('machine learning', 'NOUN'), 0.8986967),\n",
    "#  (('computer vision', 'NOUN'), 0.8636297),\n",
    "#  (('deep learning', 'NOUN'), 0.8573361)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.vocab[u'dog'].cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector1 = nlp(u\"dog\").vector\n",
    "result = nlp.vocab.vectors.most_similar(vector1.reshape(1,vector1.shape[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dir(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 684830/684830 [00:03<00:00, 218971.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import hdbscan\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "\"\"\"define all vectors and strings\"\"\"\n",
    "strings = []\n",
    "vectors = []\n",
    "for key, vector in tqdm(nlp.vocab.vectors.items(), total = len(nlp.vocab.vectors.keys())):\n",
    "    try:\n",
    "        strings.append(nlp.vocab.strings[key])\n",
    "        vectors.append(vector)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "vectors = np.vstack(vectors)\n",
    "\n",
    "\"\"\"Generate clusters, you will need to save the clusterer to use as \n",
    "clustering can take a while\"\"\"\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size = 1000)\n",
    "clusterer.fit(vectors)\n",
    "labels = clusterer.labels_\n",
    "\n",
    "\n",
    "#main function\n",
    "def closest(word, count = 10):\n",
    "    \n",
    "    word = nlp(word)\n",
    "    main = word.vector\n",
    "    \n",
    "    cluster = clusterer.fit(main).labels_\n",
    "    tmp_vectors = vectors[np.where(labels == cluster)[0]]    \n",
    "    tmp_strings = np.array(strings)[np.where(labels == cluster)[0]]\n",
    "    \n",
    "    diff = tmp_vectors - main\n",
    "    diff = diff**2\n",
    "    diff = np.sqrt(diff.sum(axis = 1), dtype = np.float64)\n",
    "   \n",
    "    df = pd.DataFrame(tmp_strings, columns = ['keyword'])\n",
    "    df['diff'] = diff\n",
    "    df = df.sort_values('diff', ascending = True).head(count)\n",
    "    df['keyword'] = df['keyword'].str.lower()\n",
    "    df = df.drop_duplicates(subset = 'keyword', keep = 'first')\n",
    "    return df\n",
    "\n",
    "closest(\"Dogs\", count = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions and Next Steps\n",
    "Summarize findings here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_cancellation_details',\n",
       " '_duration',\n",
       " '_error_json',\n",
       " '_json',\n",
       " '_no_match_details',\n",
       " '_offset',\n",
       " '_properties',\n",
       " '_reason',\n",
       " '_result_id',\n",
       " '_text',\n",
       " 'cancellation_details',\n",
       " 'duration',\n",
       " 'error_json',\n",
       " 'json',\n",
       " 'no_match_details',\n",
       " 'offset',\n",
       " 'properties',\n",
       " 'reason',\n",
       " 'result_id',\n",
       " 'text']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello hello hello hello, my name is Daniel how are you doing?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.loads(result.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DisplayText': 'Hello hello hello hello, my name is Daniel how are you doing?',\n",
       " 'Duration': 45500000,\n",
       " 'Id': '784bf512ecb64666b145fd13abad4fb2',\n",
       " 'NBest': [{'Confidence': 0.8401363,\n",
       "   'Display': 'Hello hello hello hello, my name is Daniel how are you doing?',\n",
       "   'ITN': 'hello hello hello hello my name is daniel how are you doing',\n",
       "   'Lexical': 'hello hello hello hello my name is daniel how are you doing',\n",
       "   'MaskedITN': 'hello hello hello hello my name is daniel how are you doing',\n",
       "   'Words': [{'Duration': 7700000, 'Offset': 26700000, 'Word': 'hello'},\n",
       "    {'Duration': 4600000, 'Offset': 35200000, 'Word': 'hello'},\n",
       "    {'Duration': 7400000, 'Offset': 40100000, 'Word': 'hello'},\n",
       "    {'Duration': 4800000, 'Offset': 47800000, 'Word': 'hello'},\n",
       "    {'Duration': 1300000, 'Offset': 52700000, 'Word': 'my'},\n",
       "    {'Duration': 2000000, 'Offset': 54100000, 'Word': 'name'},\n",
       "    {'Duration': 2000000, 'Offset': 56200000, 'Word': 'is'},\n",
       "    {'Duration': 5500000, 'Offset': 58300000, 'Word': 'daniel'},\n",
       "    {'Duration': 1700000, 'Offset': 64100000, 'Word': 'how'},\n",
       "    {'Duration': 1000000, 'Offset': 65900000, 'Word': 'are'},\n",
       "    {'Duration': 700000, 'Offset': 67000000, 'Word': 'you'},\n",
       "    {'Duration': 4400000, 'Offset': 67800000, 'Word': 'doing'}]},\n",
       "  {'Confidence': 0.83210075,\n",
       "   'Display': 'hello hello hello hello my name is daniel how you doing',\n",
       "   'ITN': 'hello hello hello hello my name is daniel how you doing',\n",
       "   'Lexical': 'hello hello hello hello my name is daniel how you doing',\n",
       "   'MaskedITN': 'hello hello hello hello my name is daniel how you doing',\n",
       "   'Words': [{'Duration': 7700000, 'Offset': 26700000, 'Word': 'hello'},\n",
       "    {'Duration': 4600000, 'Offset': 35200000, 'Word': 'hello'},\n",
       "    {'Duration': 7400000, 'Offset': 40100000, 'Word': 'hello'},\n",
       "    {'Duration': 4800000, 'Offset': 47800000, 'Word': 'hello'},\n",
       "    {'Duration': 1300000, 'Offset': 52700000, 'Word': 'my'},\n",
       "    {'Duration': 2000000, 'Offset': 54100000, 'Word': 'name'},\n",
       "    {'Duration': 2000000, 'Offset': 56200000, 'Word': 'is'},\n",
       "    {'Duration': 5500000, 'Offset': 58300000, 'Word': 'daniel'},\n",
       "    {'Duration': 2700000, 'Offset': 64100000, 'Word': 'how'},\n",
       "    {'Duration': 800000, 'Offset': 66900000, 'Word': 'you'},\n",
       "    {'Duration': 4400000, 'Offset': 67800000, 'Word': 'doing'}]},\n",
       "  {'Confidence': 0.8605592,\n",
       "   'Display': 'hello hello hello hello hello my name is daniel how are you doing',\n",
       "   'ITN': 'hello hello hello hello hello my name is daniel how are you doing',\n",
       "   'Lexical': 'hello hello hello hello hello my name is daniel how are you doing',\n",
       "   'MaskedITN': 'hello hello hello hello hello my name is daniel how are you doing',\n",
       "   'Words': [{'Duration': 2600000, 'Offset': 26700000, 'Word': 'hello'},\n",
       "    {'Duration': 5000000, 'Offset': 29400000, 'Word': 'hello'},\n",
       "    {'Duration': 4600000, 'Offset': 35200000, 'Word': 'hello'},\n",
       "    {'Duration': 7400000, 'Offset': 40100000, 'Word': 'hello'},\n",
       "    {'Duration': 4800000, 'Offset': 47800000, 'Word': 'hello'},\n",
       "    {'Duration': 1300000, 'Offset': 52700000, 'Word': 'my'},\n",
       "    {'Duration': 2000000, 'Offset': 54100000, 'Word': 'name'},\n",
       "    {'Duration': 2000000, 'Offset': 56200000, 'Word': 'is'},\n",
       "    {'Duration': 5500000, 'Offset': 58300000, 'Word': 'daniel'},\n",
       "    {'Duration': 1700000, 'Offset': 64100000, 'Word': 'how'},\n",
       "    {'Duration': 1000000, 'Offset': 65900000, 'Word': 'are'},\n",
       "    {'Duration': 700000, 'Offset': 67000000, 'Word': 'you'},\n",
       "    {'Duration': 4400000, 'Offset': 67800000, 'Word': 'doing'}]},\n",
       "  {'Confidence': 0.8096043,\n",
       "   'Display': \"hello hello hello hello my name is daniel how're you doing\",\n",
       "   'ITN': \"hello hello hello hello my name is daniel how're you doing\",\n",
       "   'Lexical': \"hello hello hello hello my name is daniel how're you doing\",\n",
       "   'MaskedITN': \"hello hello hello hello my name is daniel how're you doing\",\n",
       "   'Words': [{'Duration': 7700000, 'Offset': 26700000, 'Word': 'hello'},\n",
       "    {'Duration': 4400000, 'Offset': 35400000, 'Word': 'hello'},\n",
       "    {'Duration': 7400000, 'Offset': 40100000, 'Word': 'hello'},\n",
       "    {'Duration': 4800000, 'Offset': 47800000, 'Word': 'hello'},\n",
       "    {'Duration': 1300000, 'Offset': 52700000, 'Word': 'my'},\n",
       "    {'Duration': 2000000, 'Offset': 54100000, 'Word': 'name'},\n",
       "    {'Duration': 2000000, 'Offset': 56200000, 'Word': 'is'},\n",
       "    {'Duration': 5500000, 'Offset': 58300000, 'Word': 'daniel'},\n",
       "    {'Duration': 2800000, 'Offset': 64100000, 'Word': \"how're\"},\n",
       "    {'Duration': 700000, 'Offset': 67000000, 'Word': 'you'},\n",
       "    {'Duration': 4400000, 'Offset': 67800000, 'Word': 'doing'}]},\n",
       "  {'Confidence': 0.8525237,\n",
       "   'Display': 'hello hello hello hello hello my name is daniel how you doing',\n",
       "   'ITN': 'hello hello hello hello hello my name is daniel how you doing',\n",
       "   'Lexical': 'hello hello hello hello hello my name is daniel how you doing',\n",
       "   'MaskedITN': 'hello hello hello hello hello my name is daniel how you doing',\n",
       "   'Words': [{'Duration': 2600000, 'Offset': 26700000, 'Word': 'hello'},\n",
       "    {'Duration': 5000000, 'Offset': 29400000, 'Word': 'hello'},\n",
       "    {'Duration': 4600000, 'Offset': 35200000, 'Word': 'hello'},\n",
       "    {'Duration': 7400000, 'Offset': 40100000, 'Word': 'hello'},\n",
       "    {'Duration': 4800000, 'Offset': 47800000, 'Word': 'hello'},\n",
       "    {'Duration': 1300000, 'Offset': 52700000, 'Word': 'my'},\n",
       "    {'Duration': 2000000, 'Offset': 54100000, 'Word': 'name'},\n",
       "    {'Duration': 2000000, 'Offset': 56200000, 'Word': 'is'},\n",
       "    {'Duration': 5500000, 'Offset': 58300000, 'Word': 'daniel'},\n",
       "    {'Duration': 2700000, 'Offset': 64100000, 'Word': 'how'},\n",
       "    {'Duration': 800000, 'Offset': 66900000, 'Word': 'you'},\n",
       "    {'Duration': 4400000, 'Offset': 67800000, 'Word': 'doing'}]}],\n",
       " 'Offset': 26700000,\n",
       " 'RecognitionStatus': 'Success'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "228.248px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
